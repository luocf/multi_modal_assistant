#!/usr/bin/env python3
"""ç›´æ¥æµ‹è¯•transformersæ¨¡å‹åŠ è½½"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from loguru import logger

def test_direct_model():
    """ç›´æ¥æµ‹è¯•æ¨¡å‹åŠ è½½å’Œæ¨ç†"""
    try:
        model_name = "Qwen/Qwen2.5-1.5B-Instruct"
        
        logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        logger.info("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # åŠ è½½æ¨¡å‹
        logger.info("åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # ç®€å•æµ‹è¯•æ¨ç†
        logger.info("æµ‹è¯•æ¨ç†...")
        prompt = "ä½ å¥½"
        
        # ä½¿ç”¨chatæ¨¡æ¿
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text], return_tensors="pt")
        
        with torch.no_grad():
            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True
            )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        logger.info(f"âœ… æ¨ç†æˆåŠŸï¼")
        logger.info(f"ç”¨æˆ·: {prompt}")
        logger.info(f"åŠ©æ‰‹: {response}")
        
        return True
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    logger.info("=" * 50)
    logger.info("ç›´æ¥æµ‹è¯•Qwenæ¨¡å‹")
    logger.info("=" * 50)
    
    success = test_direct_model()
    
    if success:
        logger.info("ğŸ‰ æµ‹è¯•å®Œå…¨æˆåŠŸï¼")
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥") 